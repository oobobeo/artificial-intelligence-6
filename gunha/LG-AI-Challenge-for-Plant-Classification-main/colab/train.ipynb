{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "NhZM20TB8DAH"
   },
   "outputs": [],
   "source": [
    "# 개발환경(OS) : colab(Linux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "_4N2xa4IQ091"
   },
   "outputs": [],
   "source": [
    "# python==3.7.12\n",
    "# albumentations==1.1.0\n",
    "# numpy==1.19.5\n",
    "# pandas==1.3.5\n",
    "# cv2==4.1.2\n",
    "# sklearn==1.0.2\n",
    "# json==2.0.9\n",
    "# torch==1.10.0+cu111\n",
    "# timm==0.5.4\n",
    "# transformers==4.16.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "B6oNvWEsOh0w"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: opencv-python-headless 4.5.5.64\n",
      "Uninstalling opencv-python-headless-4.5.5.64:\n",
      "  Successfully uninstalled opencv-python-headless-4.5.5.64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement opencv-python-headless==4.1.2.30 (from versions: 3.4.10.37, 3.4.11.39, 3.4.11.41, 3.4.11.43, 3.4.11.45, 3.4.13.47, 3.4.14.51, 3.4.14.53, 3.4.15.55, 3.4.16.59, 3.4.17.61, 3.4.17.63, 4.3.0.38, 4.4.0.40, 4.4.0.42, 4.4.0.44, 4.4.0.46, 4.5.1.48, 4.5.2.52, 4.5.2.54, 4.5.3.56, 4.5.4.58, 4.5.4.60, 4.5.5.62, 4.5.5.64)\n",
      "ERROR: No matching distribution found for opencv-python-headless==4.1.2.30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: timm in c:\\users\\gunhakim\\anaconda3\\lib\\site-packages (0.5.4)\n",
      "Requirement already satisfied: torchvision in c:\\users\\gunhakim\\anaconda3\\lib\\site-packages (from timm) (0.12.0)\n",
      "Requirement already satisfied: torch>=1.4 in c:\\users\\gunhakim\\anaconda3\\lib\\site-packages (from timm) (1.11.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\gunhakim\\anaconda3\\lib\\site-packages (from torch>=1.4->timm) (3.10.0.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\gunhakim\\anaconda3\\lib\\site-packages (from torchvision->timm) (8.4.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\gunhakim\\anaconda3\\lib\\site-packages (from torchvision->timm) (1.20.3)\n",
      "Requirement already satisfied: requests in c:\\users\\gunhakim\\anaconda3\\lib\\site-packages (from torchvision->timm) (2.26.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\gunhakim\\anaconda3\\lib\\site-packages (from requests->torchvision->timm) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\gunhakim\\anaconda3\\lib\\site-packages (from requests->torchvision->timm) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gunhakim\\anaconda3\\lib\\site-packages (from requests->torchvision->timm) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\gunhakim\\anaconda3\\lib\\site-packages (from requests->torchvision->timm) (2.0.4)\n",
      "Requirement already satisfied: albumentations in c:\\users\\gunhakim\\anaconda3\\lib\\site-packages (1.1.0)\n",
      "Requirement already satisfied: qudida>=0.0.4 in c:\\users\\gunhakim\\anaconda3\\lib\\site-packages (from albumentations) (0.0.4)\n",
      "Collecting opencv-python-headless>=4.1.1\n",
      "  Using cached opencv_python_headless-4.5.5.64-cp36-abi3-win_amd64.whl (35.3 MB)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\gunhakim\\anaconda3\\lib\\site-packages (from albumentations) (6.0)\n",
      "Requirement already satisfied: scikit-image>=0.16.1 in c:\\users\\gunhakim\\anaconda3\\lib\\site-packages (from albumentations) (0.18.3)\n",
      "Requirement already satisfied: numpy>=1.11.1 in c:\\users\\gunhakim\\anaconda3\\lib\\site-packages (from albumentations) (1.20.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\gunhakim\\anaconda3\\lib\\site-packages (from albumentations) (1.7.1)\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in c:\\users\\gunhakim\\anaconda3\\lib\\site-packages (from qudida>=0.0.4->albumentations) (0.24.2)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\gunhakim\\anaconda3\\lib\\site-packages (from qudida>=0.0.4->albumentations) (3.10.0.2)\n",
      "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in c:\\users\\gunhakim\\anaconda3\\lib\\site-packages (from scikit-image>=0.16.1->albumentations) (3.4.3)\n",
      "Requirement already satisfied: networkx>=2.0 in c:\\users\\gunhakim\\anaconda3\\lib\\site-packages (from scikit-image>=0.16.1->albumentations) (2.6.3)\n",
      "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in c:\\users\\gunhakim\\anaconda3\\lib\\site-packages (from scikit-image>=0.16.1->albumentations) (8.4.0)\n",
      "Requirement already satisfied: imageio>=2.3.0 in c:\\users\\gunhakim\\anaconda3\\lib\\site-packages (from scikit-image>=0.16.1->albumentations) (2.9.0)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in c:\\users\\gunhakim\\anaconda3\\lib\\site-packages (from scikit-image>=0.16.1->albumentations) (2021.7.2)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in c:\\users\\gunhakim\\anaconda3\\lib\\site-packages (from scikit-image>=0.16.1->albumentations) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\gunhakim\\anaconda3\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\gunhakim\\anaconda3\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\gunhakim\\anaconda3\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (1.3.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\gunhakim\\anaconda3\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (3.0.4)\n",
      "Requirement already satisfied: six in c:\\users\\gunhakim\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (1.16.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\gunhakim\\anaconda3\\lib\\site-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\gunhakim\\anaconda3\\lib\\site-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (2.2.0)\n",
      "Installing collected packages: opencv-python-headless\n",
      "Successfully installed opencv-python-headless-4.5.5.64\n",
      "Requirement already satisfied: transformers in c:\\users\\gunhakim\\anaconda3\\lib\\site-packages (4.18.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\gunhakim\\anaconda3\\lib\\site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in c:\\users\\gunhakim\\anaconda3\\lib\\site-packages (from transformers) (0.5.1)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\gunhakim\\anaconda3\\lib\\site-packages (from transformers) (0.0.49)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\gunhakim\\anaconda3\\lib\\site-packages (from transformers) (1.20.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in c:\\users\\gunhakim\\anaconda3\\lib\\site-packages (from transformers) (0.11.6)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\gunhakim\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: requests in c:\\users\\gunhakim\\anaconda3\\lib\\site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\gunhakim\\anaconda3\\lib\\site-packages (from transformers) (21.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\gunhakim\\anaconda3\\lib\\site-packages (from transformers) (3.3.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\gunhakim\\anaconda3\\lib\\site-packages (from transformers) (2021.8.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\gunhakim\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\gunhakim\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\gunhakim\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\gunhakim\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gunhakim\\anaconda3\\lib\\site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\gunhakim\\anaconda3\\lib\\site-packages (from requests->transformers) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\gunhakim\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.7)\n",
      "Requirement already satisfied: click in c:\\users\\gunhakim\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (8.0.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\gunhakim\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (1.1.0)\n",
      "Requirement already satisfied: six in c:\\users\\gunhakim\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# pakage\n",
    "\n",
    "!pip uninstall opencv-python-headless==4.5.5.62 --yes\n",
    "!pip install opencv-python-headless==4.1.2.30\n",
    "\n",
    "import pickle\n",
    "import gc\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import random\n",
    "import os\n",
    "import json \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# timm : 0.5.4\n",
    "!pip install timm\n",
    "# albumtations : 1.1.0\n",
    "!pip install -U albumentations\n",
    "# transformers : 4.16.2\n",
    "!pip install transformers\n",
    "\n",
    "import timm\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "from albumentations import (\n",
    "    HorizontalFlip, VerticalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,\n",
    "    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n",
    "    IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, IAAPiecewiseAffine, RandomResizedCrop,\n",
    "    IAASharpen, IAAEmboss, RandomBrightnessContrast, Flip, OneOf, Compose, Normalize, Cutout, CoarseDropout, ShiftScaleRotate, CenterCrop, Resize\n",
    ")\n",
    "from transformers.optimization import AdamW, get_cosine_schedule_with_warmup\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "7uG8HgZwL4Wl"
   },
   "outputs": [],
   "source": [
    "# get sequence feature information\n",
    "\n",
    "csv_feature_dict = {'내부 습도 1 최고': [25.9, 100.0],\n",
    "                    '내부 습도 1 최저': [0.0, 100.0],\n",
    "                    '내부 습도 1 평균': [23.7, 100.0],\n",
    "                    '내부 온도 1 최고': [3.4, 47.6],\n",
    "                    '내부 온도 1 최저': [3.3, 47.0],\n",
    "                    '내부 온도 1 평균': [3.4, 47.3],\n",
    "                    '내부 이슬점 최고': [0.2, 34.7],\n",
    "                    '내부 이슬점 최저': [0.0, 34.4],\n",
    "                    '내부 이슬점 평균': [0.1, 34.5]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "y8cT9SsD5HIS"
   },
   "outputs": [],
   "source": [
    "# label encoder, decoder \n",
    "\n",
    "file_path = 'C:/Users/GunhaKim/Desktop/projects/artificial_intelligence/term_project/data'\n",
    "\n",
    "train_json = sorted(glob(f'{file_path}/train/*/*.json'))\n",
    "\n",
    "labels = []\n",
    "for i in range(len(train_json)):\n",
    "    with open(train_json[i], 'r') as f:\n",
    "        sample = json.load(f)\n",
    "        crop = sample['annotations']['crop']\n",
    "        disease = sample['annotations']['disease']\n",
    "        risk = sample['annotations']['risk']\n",
    "        label=f\"{crop}_{disease}_{risk}\" #label = crop_disease_risk\n",
    "        labels.append(label) # labels = [label1, lable2, lable3...]\n",
    "\n",
    "label_encoder = sorted(np.unique(labels)) # label_encoder : label 종류별로 나열(중복 없음)\n",
    "label_encoder = {key:value for key,value in zip(label_encoder, range(len(label_encoder)))} # label_encoder를 dictionary 형태로\n",
    "label_decoder = {val:key for key, val in label_encoder.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "F6fzFlDiXZNi"
   },
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "\n",
    "opt = dict()\n",
    "\n",
    "opt['batch_size'] = 16\n",
    "opt['class_n'] = len(label_encoder)\n",
    "opt['lr'] = 2e-4\n",
    "opt['embedding_dim'] = 512\n",
    "opt['feature_n'] = len(csv_feature_dict)\n",
    "opt['max_len'] = 300\n",
    "opt['dropout_rate'] = 0.3\n",
    "opt['epoch_n'] = 25\n",
    "opt['vision_pretrain'] = True\n",
    "opt['worker_n'] = 8\n",
    "opt['folder'] = 'model_weights'\n",
    "opt['bidirectional'] = True\n",
    "opt['minmax_dict'] = csv_feature_dict\n",
    "opt['label_dict'] = label_encoder\n",
    "opt['enc_name'] = 'resnext50_32x4d'\n",
    "opt['enc_dim'] = 2048\n",
    "opt['dec_dim'] = 1024\n",
    "opt['img_size1'] = 384\n",
    "opt['img_size2'] = 384\n",
    "opt['precision'] = 'amp'\n",
    "opt['seed'] = 42\n",
    "opt['mix'] = 'cutmix'\n",
    "opt['mix_prob'] = 0.3\n",
    "opt['mean'] = [0.485, 0.456, 0.406]\n",
    "opt['std'] = [0.229, 0.224, 0.225]\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "NwC5Ii5fwQAz"
   },
   "outputs": [],
   "source": [
    "# fix seed\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(opt['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "bbS-2KkCUd5R"
   },
   "outputs": [],
   "source": [
    "# cutmix function\n",
    "\n",
    "def rand_bbox(size, lam):\n",
    "    W = size[2] # width\n",
    "    H = size[3] # height\n",
    "    cut_rat = np.sqrt(1. - lam) # 자르는 비율, lam = np.clip(np.random.beta(alpha, alpha),0.3,0.4)\n",
    "    cut_w = np.int(W * cut_rat) # 잘라내는 이미지의 너비\n",
    "    cut_h = np.int(H * cut_rat) # 잘라내는 이미지의 높이\n",
    "\n",
    "    # uniform\n",
    "    cx = np.random.randint(W) # 잘라내는 이미지 위치 x좌표\n",
    "    cy = np.random.randint(H) # 잘라내는 이미지 위치 y좌표\n",
    "\n",
    "    #잘라내는 이미지의 4 꼭지점 좌표\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "    return bbx1, bby1, bbx2, bby2\n",
    "\n",
    "def cutmix(data, target, alpha):\n",
    "    indices = torch.randperm(data.size(0)) # randperm(n) : 0부터 n-1까지의 정수를 무작위로 나열\n",
    "    shuffled_data = data[indices] # indices를 이용해 data 랜덤한 순서로 배열\n",
    "    shuffled_target = target[indices] # indices를 이용해 target 랜덤한 순서로 배열\n",
    "\n",
    "    lam = np.clip(np.random.beta(alpha, alpha),0.3,0.4) # cutting ration를 0.3~0.4 사이로 함\n",
    "    bbx1, bby1, bbx2, bby2 = rand_bbox(data.size(), lam)\n",
    "    new_data = data.clone()\n",
    "    # new_data는 순서대로 배열되어 있는 이미지들인데 indices 순서로 배열된 이미지들과 섞고 있음\n",
    "    new_data[:, :, bby1:bby2, bbx1:bbx2] = data[indices, :, bby1:bby2, bbx1:bbx2] \n",
    "    # adjust lambda to exactly match pixel ratio(해당 이미지에서 정확히 차지하는 비율 계산)\n",
    "    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (data.size()[-1] * data.size()[-2]))\n",
    "    targets = (target, shuffled_target, lam)\n",
    "\n",
    "    return new_data, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "88ophtlaV_7g"
   },
   "outputs": [],
   "source": [
    "# get data\n",
    "\n",
    "train = sorted(glob(f'{file_path}/train/*'))\n",
    "test = sorted(glob(f'{file_path}/test/*'))\n",
    "labelsss = pd.read_csv(f'{file_path}/train.csv')['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "7Xvkqaj7GeJ5"
   },
   "outputs": [],
   "source": [
    "# k-fold cross validation\n",
    "\n",
    "skf = StratifiedKFold(n_splits = 5)\n",
    "train = shuffle(train, random_state=opt['seed'])\n",
    "\n",
    "folds = []\n",
    "for idx, (train_idx, val_idx) in enumerate(skf.split(train, labelsss)):\n",
    "    folds.append((train_idx, val_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "mvYENzv1L4Wo"
   },
   "outputs": [],
   "source": [
    "# dataset\n",
    "\n",
    "def trainTransform():\n",
    "  return Compose([\n",
    "                  #Transpose(p=0.5),\n",
    "                  #HorizontalFlip(p=0.5),\n",
    "                  VerticalFlip(p=0.5),\n",
    "                  ShiftScaleRotate(p=0.5),\n",
    "                  RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n",
    "                  Resize(opt['img_size1'], opt['img_size2']),\n",
    "                  Normalize(mean=opt['mean'], std=opt['std'], max_pixel_value=255.0, p=1.0),\n",
    "                  ToTensorV2(p=1.0),\n",
    "                  ], p=1.)\n",
    "  \n",
    "def valTransform():\n",
    "  return Compose([\n",
    "                  Resize(opt['img_size1'], opt['img_size2']),\n",
    "                  Normalize(mean=opt['mean'], std=opt['std'], max_pixel_value=255.0, p=1.0),\n",
    "                  ToTensorV2(p=1.0),\n",
    "              ], p=1.)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, opt, files, mode, transforms):\n",
    "        self.files = files\n",
    "        self.mode = mode\n",
    "        self.transforms = transforms\n",
    "        self.csv_check = [0]*len(self.files)\n",
    "        self.seq = [None]*len(self.files)\n",
    "        self.minmax_dict = opt['minmax_dict']\n",
    "        self.max_len = opt['max_len']\n",
    "        self.label_encoder = opt['label_dict']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        file = self.files[i]\n",
    "        file_name = file.split('\\\\')[-1]\n",
    "        \n",
    "        if self.csv_check[i] == 0:\n",
    "            csv_path = f'{file}/{file_name}.csv'\n",
    "            df = pd.read_csv(csv_path)\n",
    "            \n",
    "\n",
    "            try:\n",
    "                estiTime1, estiTime2 = df.iloc[0]['측정시각'], df.iloc[1]['측정시각']\n",
    "            except:\n",
    "                estiTime1, estiTime2 = 0, 1\n",
    "\n",
    "            df = df[self.minmax_dict.keys()]\n",
    "            df = df.replace('-', 0)\n",
    "            \n",
    "            if self.mode == 'train':\n",
    "                decision = np.random.rand()\n",
    "                if estiTime1==estiTime2 and len(df)>400:   # 데이터 길이가 길 때 랜덤으로 절반 뽑아오는 작업인듯?\n",
    "                    if decision > 0.5:\n",
    "                        df = df[0::2].reset_index(drop=True)\n",
    "                    else:\n",
    "                        df = df[1::2].reset_index(drop=True)\n",
    "            else: \n",
    "                if estiTime1==estiTime2 and len(df)>400:\n",
    "                    df = df[0::2].reset_index(drop=True)\n",
    "                \n",
    "            \n",
    "            # minmax-scaling\n",
    "            for col in df.columns:\n",
    "                df[col] = df[col].astype(float) - self.minmax_dict[col][0]\n",
    "                df[col] = df[col] / (self.minmax_dict[col][1]-self.minmax_dict[col][0])\n",
    "\n",
    "            # zero-padding\n",
    "            pad = np.zeros((self.max_len, len(df.columns)))\n",
    "            length = min(self.max_len, len(df))\n",
    "            pad[-length:] = df.to_numpy()[-length:]\n",
    "\n",
    "            # transpose-to-sequential-data\n",
    "            seq = torch.tensor(pad, dtype=torch.float32)\n",
    "            self.seq[i] = seq\n",
    "            self.csv_check[i] = 1\n",
    "        else:\n",
    "            seq = self.seq[i]\n",
    "        \n",
    "        # image-transform\n",
    "        image_path = f'{file}/{file_name}.jpg'\n",
    "        img = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.uint8)              \n",
    "        img = self.transforms(image=img)['image'] \n",
    "\n",
    "        \n",
    "        if self.mode == 'train' or self.mode == 'val':\n",
    "            json_path = f'{file}/{file_name}.json'\n",
    "            with open(json_path, 'r') as f:\n",
    "                json_file = json.load(f)\n",
    "            \n",
    "            crop = json_file['annotations']['crop']\n",
    "            disease = json_file['annotations']['disease']\n",
    "            risk = json_file['annotations']['risk']\n",
    "            label = torch.tensor(self.label_encoder[f'{crop}_{disease}_{risk}'], dtype=torch.long)\n",
    "            \n",
    "            return img, seq, label\n",
    "        else:\n",
    "            return img, seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "y8qmkUoRL4Wt"
   },
   "outputs": [],
   "source": [
    "# model\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, opt):\n",
    "        super(Encoder, self).__init__()\n",
    "        # resnet 50사용\n",
    "        self.model = timm.create_model(model_name=opt['enc_name'], \n",
    "                                       pretrained=opt['vision_pretrain'], \n",
    "                                       num_classes=0)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        output = self.model(inputs)\n",
    "        return output\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, opt):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.decoder = nn.GRU(opt['feature_n'], opt['embedding_dim'], bidirectional = opt['bidirectional'])\n",
    "        self.dense = nn.Linear(2*opt['max_len']*opt['embedding_dim'], opt['dec_dim'])\n",
    "        \n",
    "        self.f1 = nn.Linear(opt['enc_dim']+opt['dec_dim'], opt['enc_dim']+opt['dec_dim'])\n",
    "        self.out = nn.Linear(opt['enc_dim']+opt['dec_dim'], opt['class_n'])\n",
    "        self.dropout = nn.Dropout(opt['dropout_rate'])\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def init_weight(self):\n",
    "        torch.nn.init.xavier_uniform_(self.f1.weight)  \n",
    "        torch.nn.init.xavier_uniform_(self.dense.weight)  \n",
    "        torch.nn.init.xavier_uniform_(self.out.weight)  \n",
    "\n",
    "\n",
    "    def forward(self, enc_out, dec_inp):\n",
    "        # csv파일 해석에는 GRU사용\n",
    "        dec_out, _ = self.decoder(dec_inp)\n",
    "        dec_out = self.dense(dec_out.view(dec_out.size(0), -1))\n",
    "        \n",
    "        # encoder와 decoder로부터 나온 torch데이터를 cocat해서 fc로 돌림\n",
    "        concat = torch.cat([enc_out, dec_out], dim=1) \n",
    "        concat = self.f1(self.relu(concat))\n",
    "        concat = self.dropout(self.relu(concat))\n",
    "        output = self.out(concat)\n",
    "        return output\n",
    "\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, opt):\n",
    "        super(CustomModel, self).__init__()\n",
    "        self.encoder = Encoder(opt)\n",
    "        self.decoder = Decoder(opt)\n",
    "        self.to(device)\n",
    "        \n",
    "    def forward(self, img, seq):\n",
    "        enc_out = self.encoder(img)\n",
    "        output = self.decoder(enc_out, seq)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "PWj3yteRalRB"
   },
   "outputs": [],
   "source": [
    "# train function\n",
    "\n",
    "class CustomTrainer:\n",
    "    def __init__(self, model, folder, fold):\n",
    "        self.model=model\n",
    "        \n",
    "        self.save_dir = f'/content/{folder}'\n",
    "        if not os.path.exists(self.save_dir):\n",
    "          os.makedirs(self.save_dir)\n",
    "          \n",
    "        self.optimizer = AdamW(model.parameters(), lr=opt['lr'])\n",
    "        self.scaler = torch.cuda.amp.GradScaler() \n",
    "\n",
    "        total_steps = int(len(train_dataset)*opt['epoch_n']/(opt['batch_size']))\n",
    "        warmup_steps = 1149\n",
    "        print('total_steps: ', total_steps)\n",
    "        print('warmup_steps: ', warmup_steps)\n",
    "        self.scheduler = get_cosine_schedule_with_warmup(self.optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.val_loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.best_score = 0.0\n",
    "\n",
    "\n",
    "    def run(self, train_loader, val_loader):\n",
    "        for epoch in range(opt['epoch_n']):\n",
    "            gc.collect()\n",
    "            learning_rate = self.optimizer.param_groups[0]['lr']\n",
    "            print('learning_rate: ', learning_rate)\n",
    "            print(f'----- train, epoch{epoch+1} -----')\n",
    "            train_loss, train_score = self.train_function(train_loader, epoch)\n",
    "            print(' ')\n",
    "            print(f'train_loss: {train_loss:.6f}, train_score: {train_score:.6f}')\n",
    "\n",
    "            print('----------------------------------')\n",
    "\n",
    "            print(f'----- val, epoch{epoch+1} -----')\n",
    "            with torch.no_grad():\n",
    "                val_loss, val_score = self.val_function(val_loader)\n",
    "            print(' ')\n",
    "            print(f'val_loss: {val_loss:.6f}, val_score: {val_score:.6f}')\n",
    "\n",
    "\n",
    "            if epoch+1 >= 16 and val_score >= self.best_score:\n",
    "                torch.save(self.model.state_dict(), self.save_dir+f\"/best-acc-epoch{epoch+1}.bin\")\n",
    "                self.best_score=val_score\n",
    "                print(f'model is saved when epoch is : {epoch+1}')\n",
    "\n",
    "\n",
    "    def train_function(self, train_loader, epoch):\n",
    "        self.model.train()\n",
    "\n",
    "        total_loss = 0.0\n",
    "        total_score = 0.0\n",
    "        for bi, data in enumerate(tqdm(train_loader)):\n",
    "            data = [x.to(device) for x in data]\n",
    "            img, seq, label = data\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            # use mix or not\n",
    "            if opt['mix']!=None and epoch < opt['epoch_n']-10: \n",
    "                mix_decision = np.random.rand()\n",
    "                if opt['mix'] == 'cutmix' and mix_decision < opt['mix_prob']:\n",
    "                    img, mix_labels = cutmix(img, label, 1.0)\n",
    "                else: \n",
    "                  pass\n",
    "            else: mix_decision = 1\n",
    "\n",
    "            if opt['epoch_n']-10 <= epoch:\n",
    "                assert mix_decision == 1\n",
    "            \n",
    "            # use amp or not\n",
    "            if opt['precision'] == 'float':\n",
    "                out = self.model(img, seq)\n",
    "\n",
    "                if mix_decision < opt['mix_prob']:\n",
    "                    loss = self.loss_fn(out, mix_labels[0])*mix_labels[2] + self.loss_fn(out, mix_labels[1])*(1-mix_labels[2])\n",
    "                else:\n",
    "                    loss = self.loss_fn(out, label)\n",
    "\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "            else: \n",
    "                with torch.cuda.amp.autocast():\n",
    "                    out = self.model(img, seq)\n",
    "                    if mix_decision < opt['mix_prob']:\n",
    "                        loss = self.loss_fn(out, mix_labels[0])*mix_labels[2] + self.loss_fn(out, mix_labels[1])*(1-mix_labels[2])\n",
    "                    else:\n",
    "                        loss = self.loss_fn(out, label)\n",
    "\n",
    "                self.scaler.scale(loss).backward()  \n",
    "                self.scaler.step(self.optimizer) \n",
    "                self.scaler.update()              \n",
    "            \n",
    "            self.scheduler.step()\n",
    "            total_loss+=loss.detach().cpu()\n",
    "\n",
    "            total_score+=f1_score(label.cpu(), out.argmax(1).cpu(), average='macro')\n",
    "        return total_loss/len(train_loader), total_score/len(train_loader)\n",
    "\n",
    "    def val_function(self, val_loader):\n",
    "        self.model.eval()\n",
    "\n",
    "        total_loss = 0.0\n",
    "        preds, targets = [], []\n",
    "        for bi, data in enumerate(tqdm(val_loader)):\n",
    "            data = [x.to(device) for x in data]\n",
    "            img, seq, label = data\n",
    "\n",
    "            out = self.model(img, seq)\n",
    "            loss = self.val_loss_fn(out, label)\n",
    "\n",
    "            total_loss+=loss.detach().cpu()\n",
    "\n",
    "            pred = out.argmax(1).detach().cpu().tolist()\n",
    "            target = label.reshape(-1).detach().cpu().tolist()\n",
    "\n",
    "            preds.extend(pred)\n",
    "            targets.extend(target)\n",
    "        \n",
    "        score = f1_score(targets, preds, average='macro')\n",
    "        return total_loss/len(val_loader), score\n",
    "\n",
    "    def log(self, message):\n",
    "        with open(self.log_path, 'a+') as logger:\n",
    "            logger.write(f'{message}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "3yCjToZVTPPI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 2, 'class_n': 25, 'lr': 0.0002, 'embedding_dim': 512, 'feature_n': 9, 'max_len': 300, 'dropout_rate': 0.3, 'epoch_n': 25, 'vision_pretrain': True, 'worker_n': 8, 'folder': 'model_weights', 'bidirectional': True, 'minmax_dict': {'내부 습도 1 최고': [25.9, 100.0], '내부 습도 1 최저': [0.0, 100.0], '내부 습도 1 평균': [23.7, 100.0], '내부 온도 1 최고': [3.4, 47.6], '내부 온도 1 최저': [3.3, 47.0], '내부 온도 1 평균': [3.4, 47.3], '내부 이슬점 최고': [0.2, 34.7], '내부 이슬점 최저': [0.0, 34.4], '내부 이슬점 평균': [0.1, 34.5]}, 'label_dict': {'1_00_0': 0, '2_00_0': 1, '2_a5_2': 2, '3_00_0': 3, '3_a9_1': 4, '3_a9_2': 5, '3_a9_3': 6, '3_b3_1': 7, '3_b6_1': 8, '3_b7_1': 9, '3_b8_1': 10, '4_00_0': 11, '5_00_0': 12, '5_a7_2': 13, '5_b6_1': 14, '5_b7_1': 15, '5_b8_1': 16, '6_00_0': 17, '6_a11_1': 18, '6_a11_2': 19, '6_a12_1': 20, '6_a12_2': 21, '6_b4_1': 22, '6_b4_3': 23, '6_b5_1': 24}, 'enc_name': 'resnext50_32x4d', 'enc_dim': 2048, 'dec_dim': 1024, 'img_size1': 384, 'img_size2': 384, 'precision': 'amp', 'seed': 42, 'mix': 'cutmix', 'mix_prob': 0.3, 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}\n",
      "1th fold training is start\n",
      "num of train:  4613\n",
      "num of val:  1154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GunhaKim\\anaconda3\\envs\\projectbase\\lib\\site-packages\\transformers\\optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_steps:  57662\n",
      "warmup_steps:  1149\n",
      "learning_rate:  0.0\n",
      "----- train, epoch1 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/2306 [00:00<?, ?it/s]C:\\Users\\GunhaKim\\anaconda3\\envs\\projectbase\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "  0%|                                                                               | 1/2306 [00:02<1:42:20,  2.66s/it]C:\\Users\\GunhaKim\\anaconda3\\envs\\projectbase\\lib\\site-packages\\ipykernel_launcher.py:7: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  import sys\n",
      "C:\\Users\\GunhaKim\\anaconda3\\envs\\projectbase\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  \n",
      "  0%|                                                                               | 1/2306 [00:03<1:55:19,  3.00s/it]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 1.17 GiB (GPU 0; 8.00 GiB total capacity; 3.56 GiB already allocated; 1.07 GiB free; 4.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11464\\946017431.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;31m# trainer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mcustom_trainer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCustomTrainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfolder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'folder'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34mf'/fold{i+1}'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0mcustom_trainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11464\\875675754.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, train_loader, val_loader)\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'learning_rate: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'----- train, epoch{epoch+1} -----'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m             \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'train_loss: {train_loss:.6f}, train_score: {train_score:.6f}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11464\\875675754.py\u001b[0m in \u001b[0;36mtrain_function\u001b[1;34m(self, train_loader, epoch)\u001b[0m\n\u001b[0;32m     90\u001b[0m                         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\projectbase\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 363\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\projectbase\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m def grad(\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1.17 GiB (GPU 0; 8.00 GiB total capacity; 3.56 GiB already allocated; 1.07 GiB free; 4.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# run\n",
    "\n",
    "print(opt)\n",
    "for i in range(len(folds)):\n",
    "    train_idx, val_idx = folds[i]\n",
    "    \n",
    "    print(f'{i+1}th fold training is start')\n",
    "    \n",
    "    # data\n",
    "    train_dataset = CustomDataset(opt, np.array(train)[train_idx], mode='train', transforms=trainTransform())\n",
    "    val_dataset = CustomDataset(opt, np.array(train)[val_idx], mode='val', transforms=valTransform())\n",
    "    print('num of train: ', len(train_dataset))\n",
    "    print('num of val: ', len(val_dataset))\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=opt['batch_size'], shuffle=True, drop_last=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=2*opt['batch_size'], shuffle=False)\n",
    "\n",
    "    # model\n",
    "    custom_model = CustomModel(opt)\n",
    "\n",
    "    # trainer\n",
    "    custom_trainer = CustomTrainer(model=custom_model, folder=opt['folder']+f'/fold{i+1}', fold=i+1)\n",
    "    custom_trainer.run(train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "background_execution": "on",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "dacon-lg-code 모델 생성 및 학습 코드",
   "provenance": []
  },
  "interpreter": {
   "hash": "8207dccf39e710c758db0a3115e8b6364f9af698460a2f758c1d8836f75fc2ad"
  },
  "kernelspec": {
   "display_name": "projectbase",
   "language": "python",
   "name": "projectbase"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
